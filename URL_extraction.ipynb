{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import networkx as nx\n",
    "import nltk\n",
    "import spacy\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "from socket import timeout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Find the json data files from all available folders: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for root, dirs, files in os.walk(os.getcwd()): \n",
    "    for name in files:\n",
    "        if name.endswith((\".json\")):\n",
    "            data.append(os.path.join(root, name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load json files and make a dataframe for text messages: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = []\n",
    "for i in range(len(data)):\n",
    "    with open(data[i]) as json_file:\n",
    "        df = pd.read_json(json_file)\n",
    "        if 'text' in df.columns:\n",
    "            json_data = pd.DataFrame(df['text'])\n",
    "            all_data.append(json_data)\n",
    "all_data = pd.concat(all_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Make a dataframe from rows that contain a link (contain 'http'):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL_df = all_data[all_data['text'].str.contains('http')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Function which finds URLs for each row in the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_url(row):\n",
    "    text = str(re.sub('>', '', str(row)))\n",
    "    text = str(re.sub('<', '', text))\n",
    "    text = str(re.sub(\"'\", '', text))\n",
    "    text = str(re.sub('\"', '', text))\n",
    "    text = str(re.sub('^', '', text))\n",
    "    text = str(re.sub(';', '', text))\n",
    "    text = str(re.sub(r'\\)', '', text))\n",
    "    text = str(re.sub(r'\\[', '', text))\n",
    "    text = str(re.sub(r'\\]', '', text))\n",
    "    text = str(re.sub(r'\\n', ' ', text)) \n",
    "    text = str(re.sub(r'\\\\', ' ', text))  \n",
    "    text = str(re.sub(',', ' ', text))\n",
    "    \n",
    "    urls = re.findall('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', text)\n",
    "    print(urls)\n",
    "    return (urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying find_url function to the message texts and adding another column to the dataframe for URLs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL_df['URL'] = URL_df['text'].apply(find_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data cleaning for html pages: removing some characters from the text and split the data by comma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_clean(input):\n",
    "    output = str(input).replace('[', ',').replace(']', ',').replace(\"'\", ',').replace('(', ',').replace(')', ',')\n",
    "    output = output.replace('<','').replace('>','').replace('\"',',').replace('\"', ',').replace(r\"\\\\\", \",\")\n",
    "    output = output.split(r'\\\\|\\n|\\\\\\\\|,')\n",
    "    return [output[i] for i in range(len(output)) if len(output[i]) > 2]# ignore words with length shorter than 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading url page content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visible(element):\n",
    "    if element.parent.name in ['style', 'style', 'script', '[document]', 'head', 'title']:\n",
    "        return False\n",
    "    elif re.match('<!--.*-->', str(element.encode('utf-8'))):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def url_loader(link):   \n",
    "        \n",
    "    urlBody = []\n",
    "    try:\n",
    "        html = urllib.request.urlopen(link, timeout=10)\n",
    "    except urllib.error.URLError as e:\n",
    "        html = False\n",
    "        return False\n",
    "#     except (HTTPError, URLError) as error:\n",
    "    except InvalidURL:\n",
    "        html = False\n",
    "        return False\n",
    "    except timeout:\n",
    "        html = False\n",
    "        return False\n",
    "\n",
    "    soup = BeautifulSoup(html)\n",
    "    data = soup.findAll(text=True)\n",
    "\n",
    "    result = filter(visible, data)\n",
    "    for element in result:\n",
    "        if len(element) > 1:\n",
    "            urlBody.append((element).strip())\n",
    "            \n",
    "    urlBody = max(urlBody, key=len, default='Nan') if 'hackmd' in link else urlBody\n",
    "    cleanBody = str_clean(urlBody)\n",
    "    urls_in_body = find_url(str_clean(urlBody))\n",
    "#     print(urls_in_body)\n",
    "    return urls_in_body"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Loop over the URL column of the dataframe to pass the URLs to url_loader function and get list of urls from the page contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(URL_df['URL'].shape[0]):\n",
    "    for l in URL_df['URL'].iloc[i]:\n",
    "#         print(l)\n",
    "        url_loader(l)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
